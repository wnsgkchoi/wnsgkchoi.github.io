---
layout: post
title:  "[CSED515] 5. Support Vector Machine"

categories:
  - Machine_Learning
tags:
  - [CSED515, ML, Lecture]

toc: true
toc_sticky: true

date: 2025-10-28 00:11:00 +0900
last_modified_at: 2025-10-28 00:11:00 +0900
---

# Support Vector Machine  

## Maximum Margin Classifier  

좋은 classification hyperplane은 무엇일까? 주어진 데이터셋에 대해, 100%의 정확도로 classification을 성공하는 hyperplane은 항상 좋은 hyperplane일까? 아쉽게도 그렇지 않을 수도 있다.  

![alt text](/assets/img/CSED515/chap5/5-1.png){: width="500"}

위의 그림을 보면, 왼쪽과 오른쪽 모두 100%의 정확도로 data를 분류해내지만, 오른쪽 hyperplane이 더 좋아보인다. 그 이유는 hyperplane과 가장 가까운 데이터 사이의 거리가 멀기 때문이다. i.i.d.를 가정할 때, test set은 training dataset과 완전히 일치하지는 않더라도 어느 정도 비슷한 경향을 가질 것이다. 만약 빨간색 hyperplane을 선택한다면, test set중 일부가 miss classfy될 가능성이 파란색 hyperplane을 선택했을 때보다 높을 것이다. 따라서, 파란색과 같이 margin (hyperplane과 hyperplane과 가장 가까운 데이터 사이의 거리)이 큰 hyperplane이 더 좋은 것이라고 할 수 있겠다.  

이 간단한 아이디어를 바탕으로 다음과 같은 생각을 할 수 있다. 'margin을 최대로 만드는 hyperplane을 찾으면 어떨까?' 이런 생각을 실제 학습으로 옮기기 위해서는 margin을 수식으로 표현할 필요가 있다.  

![alt text](/assets/img/CSED515/chap5/5-2.png){: width="500"}

다음과 같이 $f(x): w^{T}x + b$ 인 hyperplane이 있고, 어떤 데이터 $x$ 가 있다고 생각해보자. 이때, 데이터 $x$ 와 hyperplane 사이의 거리는 어떻게 계산할 수 있을까?  
간단하게 언급하자면, $x_p$ 는 $x$ 를 $f$ 에 projection한 point, $|\rho_{f}(x)|$ 는 $x$ 를 $f$ 에 orthogonal projection 한 길이라고 할때,  $x = x_{p} + |\rho_{f}(x)|\frac{w}{|w|}$ 와 $f(x_{p}) = 0$을 연립하면 된다. 그리고 그 결과는 다음과 같다.  

$$
\rho_{f}(x) = \frac{f(x)}{|w|}
$$

자세한 유도는 아래에 서술해놓았다.  

<details>
<summary> margin 식 유도 </summary>
<div markdown="1">

$$x = x_{p} + |\rho_{f}(x)| \frac{w}{|w|}$$

$\frac{w}{|w|}$ 은 hyperplane에 수직인 vector 중 length가 1인 벡터다. $w$ 가 hyperplane에 수직인 vector인데, 이를 $w$ 의 크기인 $|w|$ 로 나누어 크기를 1로 만들었기 때문.  
한편, projection의 정의에 의해 $f(x_{p}) = 0$ 이어야 하므로,  

$$
<w, x - |\rho_{f}(x)|\frac{w}{|w|}> + b = 0
$$

이때, 내적은 $<a, b+c> = <a,b> + <a,c>$ 가 성립하므로,  

$$
\begin{aligned}
& <w,x> + b - |\rho_{f}(x)| \frac{<w,w>}{|w|} = 0 \\
& <w,x> + b - |\rho_{f}(x)| |w| = 0
\end{aligned}
$$

이를 $\rho_{f}(x)$ 에 대해 정리하면,  

$$
\rho_{f}(x) = \frac{<w,x> + b}{|w|} = \frac{|f(x)|}{|w|}  
$$

</div>
</details>

<br>  

margin을 식으로 정의했으니, 이제 objective를 설정할 수 있겠다. 이는 다음과 같이 나타낼 수 있다.  

<div class="callout">
  <div class="callout-header">margin maximization</div>
  <div class="callout-body" markdown="1">

sample이 linearly separable할 때,  

$$
\rho = \max_{f: y_i f(x_i) \geq 0} \min_{i} \rho_{f}(x_{i}) = \max_{f: y_i f(x_i) \geq 0} \rho_{f}
$$

  </div>
</div>
<br>

즉, 제대로 분류된 데이터에 대해($f: y_i f(x_i) \geq 0$), 가장 가까운 데이터의 margin을 최대화하는 것이다. 이때, 위의 식을 다음과 같이 변형할 수 있다.  

$$
\rho = \max_{f}\min_{i} \frac{y_{i}f(x_{i})}{|w|} = \max_{f}\min_{i} \frac{y_{i}(w^{T}x_{i} + b)}{|w|}
$$

만약 모든 데이터가 잘 분류되었다면, 
$y\_{i}f(x\_{i})$
는 항상 양수일 것이며, 
$y\_{i}$ 
는 크기가 1이므로, 
$\frac{y\_{i}f(x\_{i})}{|w|}$ 
는 잘 분류된 경우 margin과 같고, 잘 분류되지 않은 경우 - margin이 된다. 이때, max min이므로, 정상적으로 분류되지 않은 데이터가 있다면, min 에 의해 음수가 select되고, max가 존재하므로, linearly separable한 데이터셋에서, 100%의 정확도를 가지지 못하는 hyperplane은 자연스럽게 선택되지 않게 된다.  

그런데 사실 어떤 양수 $\gamma$ 가 주어질때, $(\gamma w, \gamma b)$는 $(w, b)$ 와 같은 hyperplane을 가진다. 식으로 표현하면 다음과 같다.  

$$
\left\{ x | w^{T}x + b = 0 \right\} = \left\{ x | \gamma(w^{T}x + b) = 0 \right\}
$$

이 점을 고려할 때, 우리는 다음과 같이 $\rho$ 를 구할 수도 있다.  

$$
\rho = \max_{f: \min_{i}y_{i}(w^{T}x_{i}+ b) = 1} \frac{1}{|w|} = \max_{f: \forall i, y_{i}(w^{T}x_{i} + b) \geq 1} \frac{1}{|w|}
$$

이 식을 천천히 살펴보자. 위의 식에서는 
$|w|$ 
의 크기를 통일하여 가장 margin이 큰 hyperplane을 찾았다면, 이번에는 
$w$ 
의 크기는 제한하지 않되, margin이 1이 되도록 하는 모든 
$w$
에 대해, 
$|w|$ 
이 가장 작은 hyperplane을 뽑는다. 
$|w|$ 
의 크기가 작은데, 
$w^{T}x_{t} + b$ 
의 값이 같다면 실제로 margin이 크다는 뜻이 되기 때문에, 결과적으로 이 식은 위에서 살펴본 식과 동일한 뜻을 가지게 된다.  

이제 이 식으로 학습을 해야 하는데, 어떻게 학습을 할 수 있을까? maximizing margin은 다음과 같이 formulate될 수 있다.  

$$
\begin{aligned}
& \min_{w} \frac{1}{2} |w|^{2} \\
& \text{subject to } y_{i}(w^{T}x_{i} + b) \geq 1 \quad \forall i = 1, \ldots, N
\end{aligned}
$$

![alt text](/assets/img/CSED515/chap5/5-3.png){: width="500"}

위의 그림과 함께 보면 수식 이해가 수월하다. 모든 데이터에 대해, margin을 1 이상으로 만드는 parameter $w$ 중에서, 가장 작은 크기의 $w$ 를 찾는 것이다. 이 solution을 SVM이라고 부른다.  

## Constrained Optimization  

다음과 같은 objective function $ L: R^{D} \rightarrow R$ 에 대해 생각해보자.  

$$
\begin{aligned}
& \min_{x} L(x) \\
& \text{subject to} \quad g_{i}(x) \leq 0 \quad \forall i = 1, \ldots, m
\end{aligned}
$$

where $g_{i}: R^{D} \rightarrow R$  

이런 형태로 나타낸 것을 **primal problem**이라 부른다. 다만 이러한 최적화 문제는 optimize하기 어려운데, 그 이유는 당연히 많은 제약조건 때문이다. 따라서, Constrained에서 Unconstrained problem으로 바꾸는 것이 좋다. 가장 먼저 생각해볼 수 있는 방법은, infinite step function을 도입하는 것이다.  

infinite step function은 다음과 같이 정의된다.  

$$
\mathbb{1}(z) = \begin{cases}
                    0 & \text{if } z \leq 0 \\
                    \infty & \text{otherwise}
                \end{cases}
$$

loss function에 $g_{i}(x)$ 가 조건을 만족하면 0, 아니면 $\infty$ 을 반환하도록 하는 이 함수를 추가하면 어떨까? 만약 조건을 만족하지 않는 경우가 존재한다면 loss가 $\infty$ 이 되어 자연스럽게 해당 방향으로 최적화를 안 하게 될 것이다. 이 아이디어를 기반으로 다음과 같은 loss function을 사용할 수 있다.  

$$
J(x) = L(x) + \sum_{i=1}^{m} \mathbb{1}(g_{i}(x))
$$

하지만, 이런 infinite step function은 다음 두 가지 문제에 귀속된다.  

1. gradient 정보가 없다.  
  이는 GD를 사용하기 힘들게 한다. Constraint를 위반했다는 사실은 알 수 있지만, 어느 방향으로 가야 이 Constraint를 만족시킬 수 있을지에 대한 정보를 얻을 수 없다.  
2. Constraint 위반 정도에 대한 정보가 없다.  
  Constraint를 위반한다면 그 정도에 상관 없이 항상 $\infty$ 을 반환하므로 알고리즘이 효율적으로 해를 찾기 힘들다. 예를 들어, $g(x) \leq 0$ 이 제약 조건일 때, $g(x) = 0.001$ 인 경우와 $g(x) = 100$ 인 경우 모두 제약 조건을 위반하지만, 명백히 $ g(x) = 0.001$ 인 경우가 최적화 관점에서 더 나은 상태다. 하지만, infinite step function은 이에 대한 정보를 제공할 수 없다.  

따라서, Lagrangian relaxation을 사용하여 식을 재정의한다.  

<div class="callout">
  <div class="callout-header">Lagrangian Relaxation</div>
  <div class="callout-body" markdown="1">

The Lagrangian relaxation or (simply) Lagrangian of the primal problem has the following form with $\lambda_{i} \geq 0$:  

$$
\mathcal{L} (x, \lambda) = L(x) + \sum_{i=1} \lambda_{i} g_{i}(x)
$$

  </div>
</div>
<br>

이 식이 왜 좋은 approximation이 될 수 있을까? 바로, 이 식에 $\max$ 를 추가하면 infinite step function과 같은 효과를 냄과 동시에 위의 두 문제를 해결하기 때문이다.  

$$
J(x) = L(x) + \sum_{i=1}^{m}\mathbb{1}(g_{i}(x)) = L(x) + \max_{\lambda \geq 0 } \sum_{i=1}^{m}\lambda_{i}g_{i}(x) = \max_{\lambda \geq 0} \mathcal{L} (x, \lambda)
$$

이제 Dual에 대해 알아보자. Dual은 최적화 문제를 표기하는 또 다른 방법으로, primal과 어느 정도 연관이 있다. (이 연관에 대해서는 아래에서 다룬다.)  

<div class="callout">
  <div class="callout-header">Lagrangian Dual</div>
  <div class="callout-body" markdown="1">

The Lagrangian dual of the primal problem is given by

$$
\max_{\lambda \geq 0} \mathcal{D}(\lambda)
$$

where $\mathcal{D}(\lambda) = \min_{x \in R^{d}} \mathcal{L}(x,\lambda)$

  </div>
</div>
<br>

그래서, Lagrangian Dual이 primal problem과 어떤 관계일까? 이를 알기 위해 먼저 **Minimax Inequality**에 대해 알아야 한다.  

<div class="callout">
  <div class="callout-header">Minimax Inequality</div>
  <div class="callout-body" markdown="1">

$$
\max_{y} \min_{x} \rho (x, y) \leq \min_{x} \max_{y} \rho (x, y)
$$

  </div>
</div>
<br>

대충 말로 풀어 설명하자면, 어떤 function $\rho$ 가 어떤 variable에 대해서는 최소, 어떤 variable에 대해서는 최대가 될 때, 최댓값을 최소화시키는 것이 최솟값을 최대화시키는 것보다 항상 크거나 같다는 뜻이다. 이에 대한 증명은 다음과 같다.  

![proof of minimax inequality](/assets/img/CSED515/chap5/5-4.png){: width="1500"}

증명 자체는 간단하다. 첫 번째 inequality는 definition of $\psi$ 에 의해 성립한다. 두 번째 inequality는 첫 번째 inequality가 모든 y에 대해 성립하므로, 각 항에 $\max_{y}$ 를 해도 성립하는 것이다. 세 번째 inequality는 두 번째 inequality가 모든 x에 대해 성립하므로, x와 관련 있는 우변에 $min_{x}$ 를 취해도 성립하게 된다. 마지막 inequality는 $\psi$ 의 정의에 의해 성립하게 된다.  

이제 이 Minimax inequality를 적용하면 primal solution과 dual solution에 다음과 같은 weak duality가 성립함을 알 수 있다.  

$$
min_{x} J(x) = \underbrace{\min_{x} \max_{\lambda \geq 0} \mathcal{L} (x, \lambda)}_{\text{Primal solution}} \geq \underbrace{\max_{\lambda \geq 0}\min_{x} \mathcal{L} (x, \lambda)}_{\text{Dual solution}}
$$

우리의 optimization problem의 해는 primal solution이다. 하지만, 이 weak duality에 의해, Lagrangian dual을 풀면, primal problem의 lower bound를 얻을 수 있게 된다. 참고로, weak duality는 equality가 성립하지 않을 수 있는 dual을 뜻하고, strong duality는 equlity가 성립하는 dual을 뜻한다.  
여기에서 하나의 의문이 들 수 있다. primal solution과 dual solution은 일치하지 않을 수 있고, dual solution은 일종의 근사치가 될텐데, 왜 dual problem을 굳이 배우는가. 이는 바로, dual이 unconstrained problem이기 때문이다. 
일단 $\min_{x} \mathcal{L}(x, \lambda)$ 는 $\lambda$ 에 대해 풀기 어렵지 않다. 따라서 primal problem보다 계산에 있어 이점이 있다.  

지금까지는 inequality constraint에 대해서만 다루었는데, 간혹 equality constraint도 존재할 수 있다. 이런 경우 어떻게 처리할 수 있을까? 일종의 inequality처럼 처리하는 방법이 있고, equality를 따로 처리하는 방법이 있다. 사실 두 방법이 크게 차이가 없긴 하다.  
Equality Constraints를 $h(x) = 0$ 으로 표현하자. 먼저, inequality로 처리하는 경우를 생각해보자. 등호가 성립할 때를 제외하면 constraint를 위반하게 되므로, 다음과 같이 두 개의 부등식으로 constraint를 재정의할 수 있다.  

$$
\begin{aligned}
& h(x) \leq 0 \\
& h(x) \geq 0 \Rightarrow -h(x) \leq 0
\end{aligned}
$$

이 식을 loss function에 추가하면 다음과 같아진다.  

$$
\mathcal{L}(x, \lambda) = L(x) + \lambda_{1}h(x) - \lambda_{2}h(x), \quad \lambda_{1} , \lambda_{2} \geq 0
$$

이 식에서 끝내도 사실 상관은 없다. 하지만, lagrangian multiplier가 0보다 큰 수여야 한다는 제약조건을 포기한다면 다음과 같이 식을 변형할 수 있다. (이 경우가 equality를 따로 처리하는 경우다.)  

$$
\mathcal{L}(x, \lambda) = L(x) - \lambda h(x), \quad \lambda \in \mathbb{R}
$$

이 강의노트에서는 항상 inequality와 equality를 따로 처리하는 방법을 사용한다. 그렇다면 Equality Constraint를 포함하는 Lagrangian에 대해 다음과 같이 일반화할 수 있다.  

$$
\begin{aligned}
\text{minimize} L(w) \\
\text{subject to } g_{i} \leq 0, \quad i = 1, \ldots, M,
h_{j}(w) = 0, \quad j = 1, \ldots, L.
\end{aligned}
$$

이때 Lagrangian은 다음과 같이 주어진다.  

$$
\mathcal{L}(w, v, \lambda) = L(w) + \sum_{i=1}^{M} v_{i}g_{i}(w) + \sum_{j=1}^{L}\lambda_{j}h_{j}(w)
$$

where $v_{i} \geq 0$ for $ i= 1, \ldots, M$ and $\lambda_{j} \in \mathbb{R}$ for $j = 1, \ldots, L.$  

이제 SVM으로 넘어가기 전에, 꽤 중요한 조건을 하나 다루고 넘어가자. **Karush-Kuhn-Tucker (KKT) Conditions**이다.  
먼저 가정이 필요하다. Primal problem이 convex해야 한다. 이떄, $(w, \nu, \lambda)$ 가 다음 세 가지 조건을 만족한다고 가정하자.  

1. Optimality:  

$$
\nabla_{w}\mathcal{L}(w, \nu, \lambda) = \nabla L(w) + \sum_{i=1}^{M} \nu_{i} \nabla g_{i}(w) + \sum_{j=1}^{L} \lambda_{j} \nabla h_{j}(w) = 0
$$

2. Feasibility:  

$$
\begin{aligned}
& g_{i}(w) \leq 0, \quad i = 1, \ldots, M \\
& h_{j}(w) = 0, \quad j = 1, \ldots, L
\end{aligned}
$$

3. Complementary slackness:  

$$
\nu_{i}g_{i}(w) = 0, \quad i = 1, \ldots, M \quad (\nu_{i} \geq 0)
$$

이때, $w$ 은 primal optimal하며, $(\nu, \lambda)$ 는 zero duality gap을 가지는 dual optimal이다. 즉, convex problem에서 primal optimal value와 dual optimal value가 같다.

각각의 조건에 대해 생각해보고 넘어가자.  
먼저, Optimality의 경우, 주어진 parameter tuple에 대해, loss function의 gradient가 0이 되어야 함을 의미한다. 즉, optimal함을 뜻한다. (Convex)  
두 번째 조건은 모든 constraint를 만족함을 의미한다. 다시 말해, constraint에 위배되지 않는 parameter $w$를 뜻한다.  
마지막 조건은 Constraint로 인한 loss가 0이 되어야 함을 의미한다. 즉, constraint의 값이 0이 아니면, lagrangian multiplier가 0이 되어야 하고, 만약 constraint의 값이 0이 되면, lagrangian multiplier의 값이 상관 없는 그런 조건이다. 조금 더 나아가 생각해보면, constraint를 안정적으로 만족하는 경우, 제약조건이 있으나 마나 큰 상관이 없으므로, 중요도를 나타내는 Lagrangian multiplier가 0이 되지만, Constraint의 값이 정확히 0이 되는 경우, 이 제약 조건은 optimal solution의 위치에 중요한 영향을 끼치게 되므로, Lagrangian multiplier가 0보다 크게 될 수 있다. 이 조건은 향후 SVM에서 중요한 역할을 하게 된다.  

## Diving into Support Vector Machines (SVMs)  

### Linearly Separable Case  

이제 이 primal problem과 dual problem의 개념을 SVM에 적용해보자. 먼저, Primal problem은 다음과 같이 정의할 수 있다.  

<div class="callout">
  <div class="callout-header">Primal Problem of SVM</div>
  <div class="callout-body" markdown="1">

$$
\begin{aligned}
& \min_{w \in \mathbb{R}^{D}, b \in \mathbb{R}} \frac{1}{2} |w|^{2} \\
& \text{subject to } y_{i} \left( w^{T}x_{i} + b \right) \geq 1, \quad i = 1, \ldots, N
\end{aligned}
$$

  </div>
</div>
<br>

이에 대한 Dual Problem을 정의하기 위해, Lagrangian multiplier를 도입하여 식을 변형해보도록 하자.  

<div class="callout">
  <div class="callout-header">Primal Lagrangian</div>
  <div class="callout-body" markdown="1">

$$
\max_{\alpha \geq 0} \min_{w, b} \mathcal{L}(w, b, \alpha)
$$

where

$$
\mathcal{L}(w, b, \alpha) = \frac{1}{2} |w|^{2} + \sum_{i=1}^{N} \alpha_{i}(1 - y_{i}(w^{T}x_{i} + b))
$$

  </div>
</div>
<br>

이때, $ \alpha _{i}$ 는 Lagrangian multiplier다. 이 식의 optimal solution은 어떻게 찾을 수 있을까. 여기에서 KKT condition을 사용한다. 일단, optimal solution이려면, $w$와 $b$ 에 대한 gradient가 0이어야 한다. 이를 사용해 식을 풀면,  

$$
\begin{aligned}
& \frac{\partial \mathcal{L}}{\partial w} = 0 \Rightarrow w* = \sum_{i=1}^{N} \alpha_{i}y_{i}x_{i} \\
& \frac{\partial \mathcal{L}}{\partial b} = 0 \Rightarrow \sum_{i=1}^{N} \alpha_{i}y_{i} = 0
\end{aligned}
$$

또, KKT의 세 번째 조건에 의해,  

$$
\alpha_{i}(1 - y_{i} (w^{T}x_{i} + b)) = 0 \quad \forall i
$$

이 중 위의 두 개의 조건부터 사용하여 식을 변형해보자.  

$$
\min_{w, b} \mathcal{L} (w, b, \alpha) = \mathcal{L} (w*, b*, \alpha) = - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j} + \sum_{i=1}^{N} \alpha_{i}
$$

이제 Primal Problem은 다음 dual problem으로 대체될 수 있다.  

<div class="callout">
  <div class="callout-header">Dual Problem</div>
  <div class="callout-body" markdown="1">

$$
\begin{aligned}
& \max_{\alpha \geq 0} - \frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N}\alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j} + \sum_{i=1}^{N} \alpha_{i} \\
& \text{subject to} \quad \sum_{i=1}^{N} \alpha_{i}y_{i} = 0
\end{aligned}
$$

  </div>
</div>
<br>

이때, 주어진 linearly separable training samples 
$\left\\{(x\_{i}, y\_{i}) \right\\}\_{i=1}^{N}$ 
에 대해, 
$\alpha^{\*}$ 
가 dual problem의 최적해라고 가정해보자. 그렇다면, 
$w^{\*} = \sum\_{i=1}^{N} \alpha\_{i}y\_{i}x\_{i}$ 
와 
$b^{\*} = y\_{i} - \sum\_{j=1}^{N} \alpha\_{j}y\_{j}x\_{j}^{T}x\_{i}$ 
는 geometric margin이 
$\rho = \frac{1}{|w|}$ 
인 maximal margin hyperplane을 형성한다.  
여기에서, 
$w^{\*}$ 
는 위에서 구한 KKT Condition에서 온 것이다. $b^{\*}$ 의 경우, KKT condition을 구할 때 gradient를 구하는 과정에서 사라졌는데, 이를 세 번째 조건과 첫 번째 조건을 결합하여 유도한다. 세 번째 조건은 
$\alpha\_{i}(1 - y\_{i} (w^{T}x\_{i} + b)) = 0 \quad \forall i$ 
였다. 세 번째 조건이 성립하기 위해서는 
$\alpha\_{i} = 0$ 
이거나, 
$(1 - y\_{i} (w^{T}x\_{i} + b)) = 0$ 
이어야 한다. 이때, 
$1 - y\_{i} (w^{T}x\_{i} + b) = 0$ 
를 b에 대해 정리한 뒤, 
$w$ 
에 
$w^{\*}$ 
를 대입한다. 그러면 위와 같은 
$b^{\*}$ 가 나오게 된다.  

이 dual problem의 optimal solution은 margin을 최대로 하는 hyperplane을 형성한다. 이때, support vector란, 이전에 살펴본 조건 중 사용하지 않은 조건인 
$ \alpha\_{i}(1 - y\_{i} (w^{T}x\_{i} + b)) = 0 \quad \forall i$ 
에서, 
$\alpha\_{i} $ 
가 0이지 않아도 되는 vector, 즉, 
$(1 - y\_{i} (w^{T}x\_{i} + b)) = 0$ 
을 만족시키는 vector로, 이 vector만이 오직 decision boundary에 영향을 주므로 이런 이름이 붙었다.  

Primal과 Dual 중 어느 문제를 푸는 것이 좋은지는 데이터셋에 달렸다. 만약 데이터셋의 개수인 $ N$ 이 차원 $ D$ 보다 훨씬 크다면, primal problem을 바로 푸는 것이 좋다. 반대로, 데이터의 개수보다 차원이 크거나, non-linear basis function (kernel)을 사용한다면 dual problem을 푸는 것이 좋다.  

### Linearly Non-Separable Case  

지금까지 우리는 data가 linearly separable하다고 가정했다. 하지만, data가 linearly separable하지 않다면 어떨까? 어떤 boundary에 대해, mis-classified된 data를 처리하기 위해, slack variable $ \xi$를 도입한다. 이 slack variable을 추가한 constraint는 다음과 같다.  

$$
y_{n}\left( w^{T}x_{n} + b \right) \geq 1 - \xi_{n} \quad n = 1, \ldots, N
$$

Slack variable을 추가한 SVM을 Primal Problem으로 정의하면 다음과 같아진다.  

$$
\begin{aligned}
& \min_{w, b, \xi} \frac{1}{2} |w|^{2} + C \sum_{i=1}^{N}\xi_{i} \\
& \text{subject to} \quad y_{i}(w^{T}x_{i} + b) \geq 1 - \xi_{i} \quad \text{for } i = 1, \ldots, N \\
& \quad\quad\quad\quad \xi_{i} \geq 0 \quad \text{for } i = 1, \ldots, N.
\end{aligned}
$$

이 primal problem의 dual은 다음과 같다.  

$$
\max_{\alpha, \beta \geq 0} \min_{w, b, \xi \geq 0} \mathcal{L}(w, b, \xi, \alpha, \beta)
$$

where

$$
\mathcal{L}(w, b, \xi, \alpha, \beta) = \frac{1}{2} |w|^{2} + \frac{C}{2} \sum_{i=1}^{N} \xi^{2} + \sum_{i=1}^{N} \alpha_{i} (1 - \xi_{i} - y_{i}(w^{T}x_{i} + b)) - \sum_{i=1}^{N}\beta_{i}\xi_{i}
$$

이전과 마찬가지로 KKT condition을 사용하면 다음과 같은 결론을 얻을 수 있다.  

$$
\begin{aligned}
& \frac{\partial \mathcal{L}(w, b, \xi, \alpha, \beta)}{\partial w} = 0 \Rightarrow w = \sum_{i=1}^{N} \alpha_{i}y_{i}x_{i} \\
& \frac{\partial \mathcal{L}(w, b, \xi, \alpha, \beta)}{\partial b} = 0 \Rightarrow \sum_{i=1}^{N} \alpha_{i}y_{i} = 0 \\
& \frac{\partial \mathcal{L}(w, b, \xi, \alpha, \beta)}{\partial \xi} = 0 \Rightarrow \alpha_{i} + \beta_{i} = C \\
& \alpha_{i}(y_{i}(w^{T}x_{i} + b) - 1 + \xi_{i}) = 0 \quad \forall i \\
& \beta_{i}\xi_{i} = 0 \quad \forall i
\end{aligned}
$$

이 결론을 원래 식에 대입하면,  

$$
\min_{w, b, \xi \geq 0} \mathcal{L}(w, b, \xi, \alpha, \beta) = \underbrace{\frac{1}{2} \left| \sum_{i=1}^{N} \alpha_{i}y_{i}x_{i} \right|^{2} - \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}}_{-\frac{1}{2}\sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j}} - \underbrace{\sum_{i=1}^{N} \alpha_{i}y_{i}b}_{0} + \sum_{i=1}^{N} \alpha_{i}
$$

식을 정리하여 Dual Problem을 정의하면 다음과 같다.  

$$
\begin{aligned}
& \max_{\alpha} \quad -\frac{1}{2} \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{i}\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j} + \sum_{i=1}^{N} \alpha_{i} \\
& \text{subject to} \quad 0 \leq \alpha_{i} \leq C \text{ and } \sum_{i=1}^{N} \alpha_{i=1}^{N}\alpha_{i}y_{i} = 0 \quad \forall i  
\end{aligned}
$$

이때, slack variable $\xi_{n}$ 이 0보다 클 때, 우리는 이것을 일종의 loss로 볼 수 있다고 한다. hinge loss를 떠올려보면, $\ell_{\text{hinge}}(t_{i}) = \max\{ 0, 1 - t_{i} \}$ 으로 나타낼 수 있는데, slack variable $\xi_{i}$ 이 정확히 같은 form으로 나타내지기 때문이다. $\xi_{i} \geq 1 - y_{i}(w^{T}x_{i} + b)$  

이제 Primal Problem을 다음과 같이 hinge loss를 적용하여 나타낼 수 있다.  

$$
\min_{w, b} \frac{1}{2} |w|^{2} + C \sum_{i}^{N} \ell_{\text{hinge}} (y_{i} (w^{T}x_{i} + b))
$$

## SVMs v.s. logistic regression  

다른 loss들과 Binary SVM을 비교해보면, 수식의 형식이 비슷하다.  

![Fig 5. loss summary](/assets/img/CSED515/chap5/5-5.png){: width="500"}

![Fig 6. loss plot](/assets/img/CSED515/chap5/5-6.png){: width="500"}

이 loss들을 다음과 같이 일반화할 수 있다.  

$$
\min_{w, b} \frac{C'}{2}|w|^{2} + \sum_{i=1}^{N} \epsilon \log \left( 1 + \exp \left( \frac{L - y_{i}w^{T}\phi(x_{i})}{\epsilon} \right) \right)
$$

이 식에서, $\epsilon$ 과 $L$ 의 값을 조절하여 다양한 loss를 표현할 수 있다. 예를 들어, $\epsilon = 1$, $L = 0$ 으로 설정하면 Logit regression과 완벽히 일치하게 된다. 반대로, $L = 1$, $\epsilon \rightarrow 0$ 으로 설정하면, Binary SVM으로 수렴하게 된다.  

## Kernel Trick  

대충 feature function의 dimension이 클 때, corresponding kernel이 있으면 더 쉽게 계산이 가능하다는 정도로 요약하고 넘어가면 될 듯.