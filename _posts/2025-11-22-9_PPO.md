---
layout: post
title:  "[CSED627] 9. PPO"

categories:
  - Reinforcement_Learning
tags:
  - [CSED627, Lecture, paper]

toc: true
toc_sticky: true

date: 2025-11-22 21:45:00 +0900
last_modified_at: 2025-11-22 21:45:00 +0900
---

> CSED627 강의의 아홉 번째 topic으로 PPO을 배웠는데, 지금 연구하려는 분야와 겹치는 부분이 있어서 다시 리뷰하려 한다. 그리고 일단 리뷰하는 김에, PPO가 처음 소개된 논문까지 같이 리뷰할 생각이다.  

## 1. Intro  

강화학습은 고차원 state space와 복잡한 action space를 가진 문제들을 해결하는 데 있어 획기적인 진전을 이루어왔다. 특히 로봇 제어, 자율 주행, 그리고 최근의 LLM의 인간 피드백 기반 강화학습 (RLHF, 곧 다룰 예정)에 이르기까지, Policy Gradient 계열의 방법론은 그 중심에 서 있다. 그러나 초기의 Policy Gradient 방법론, 예를 들어 REINFORCE나 Vanilla Policy Gradient는 학습의 불안정성이라는 치명적인 단점을 안고 있었다. 이는 주로 policy update의 step size를 적절히 조절하지 못해 발생하는 문제로, 너무 작은 step은 학습을 지나치게 느리게 만들고, 너무 큰 step은 policy를 복구 불가능한 수준으로 망가뜨리는 'Performance Collapse'를 야기했다.

이러한 배경에서 등장한 Trust Region Policy Optimization (TRPO)는 수학적으로 단조 향상(Monotonic Improvement)을 보장하는 이론적 틀을 제시했다. TRPO는 policy 변경의 크기를 Kullback-Liebler divergence (KL-divergence)으로 제약함으로써 안정성을 확보했다. 하지만 TRPO는 2차 미분 정보인 Fisher informatoin Matrix를 계산하거나 근사해야 하므로 Computational Complexity가 매우 높고 구현이 까다롭다는 실용적인 한계를 안고 있었다.  

2017년 OpenAI의 Schulamn 등이 제안한 Proximal Policy Optimization (PPO)는 TRPO의 이론적 이점을 계승하면서도, 복잡한 2차 최적화 과정을 1차 미분 기반의 Clipping 기법으로 대체하여 구현의 용이성과 computational efficiency를 동시에 달성했다. 본 글은 PPO 알고리즘의 작동 원리를 심층적으로 분석한다. Policy Gradient Theorem, Performance Difference lemma, 그리고 PPO의 핵심인 Clipped Surrogate Objective와 Generalized Advantage Estimator(GAE)의 유도 과정을 수학적으로 엄밀히 다룰 계획이다. 또한, 실제 구현 시 성능에 결정적인 영향을 미치는 코드 레벨의 최적화 기법들까지 포괄적으로 탐구하며, 이론과 실제의 간극을 메우는 것을 목표로 한다.  

## 2. Policy Gradient Theorem  

PPO 알고리즘의 근간이 되는 Policy Gradient Theorem을 Sutton & Barto의 접근법에 따라 엄밀히 유도해본다.  
목표는 성과 함수 
$J(\theta) = V^{\pi_{\theta}}(s_{0})$ 
의 파라미터 
$\theta$ 
에 대한 gradient 
$\nabla_{\theta} J(\theta)$ 
를 구하는 것이다.  

### 2.1. Value function의 gradient 전개  

