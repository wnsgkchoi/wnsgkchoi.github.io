---
layout: post
title:  "[CSED627] 3. MDP"

categories:
  - Lecture
tags:
  - [CSED627, Lecture]

toc: true
toc_sticky: true

date: 2025-09-10 02:15:41 +0900
last_modified_at: 2025-09-10 02:15:41 +0900
---

# Markov Decision Process  

```text
아니 어떻게 수강신청 확정 후 처음 들은 강화학습 수업이 이렇게 빡셀 수가 있지..?  
다행인지 아닌지 모르겠지만, 증명은 시험에 안 나온다는 말을 들은 듯하다. 대신 대학원생이라면 증명을 한 번 쯤 스스로 해보면 좋다고 말씀하셨다.  
```

## What is MDP?  

MDP는 MRP에 action이 더해진 것으로, 다섯 개의 element를 가지는 tuple로 정의할 수 있다.  

$$(S, A, P, R, \gamma)$$

각 element는 다음을 의미한다.  

- $S$: set of states  
- $A$: set of actions  
- $P$: dynamics/transition model for each action, $P(s_{t+1} = s' | s_t = s, a_t = a)$  
- $R$: reward function $R(s_t = s, a_t = a) = \mathbb{E} [r_t | s_t = s, a_t = a]$  
- $\gamma$: discount factor, $\gamma \in [0, 1]$  

### Policy  

특정 state에서 어떤 action을 취할 것인지 결정한다. 이때, policy는 deterministic할 수도 있지만, stochastic할 수도 있다. policy를 식으로 나타내기 위해, 일종의 conditional distribution이라고 생각하면, 아래와 같이 나타낼 수 있다.  
$$
\pi (a|s) = P \left( a_t = a | s_t = s \right)
$$

MDP와 policy를 결합하면 MRP로 나타낼 수 있다. MRP는 $\left( S, P, R, \gamma \right)$ 로 나타낼 수 있는데, 다음과 같이 MDP + policy를 나타낼 수 있다.  
$\left( S, P^{\pi}, R^{\pi}, \gamma \right)$, where  
$$
R^{\pi}(s) = \sum_{a \in A} \pi (a | s)R(s, a) \\
P^{\pi}(s' | s) = \sum_{a \in A} \pi (a | s)P(s' | s, a)
$$

굳이 해석을 하자면, MRP에서 $R$은 reward function으로, 해당 state에서 얻을 수 있는 reward의 평균을 나타내는 것이었다. 위의 식을 보면, 어떤 action을 선택할 확률 $\pi (a|s)$과 해당 action을 선택했을 때 얻을 수 있는 reward의 평균인 변량 $R(s,a)$을 sum한 것이므로, 이 정의와 완벽히 일치함을 알 수 있다.  
$P$는 transition model로, 다음 step에 이동할 state를 결정한다. 위의 수식을 보면 특정 action을 선택할 확률과, 해당 action을 선택했을 때의 transition model을 곱한 것을 sum했으므로, 정의와 일치함을 알 수 있다.  

### Policy evaluation: iterative algorithm  

이제 어떤 policy가 주어질 때, 이 policy가 좋은 policy인지 평가하는 방법에 대해 알아볼 것이다. 이에 앞서, 몇 가지 fucntion을 정의하도록 하자.  
MRP에서 $V_k (s)$ 는 value function으로, 해당 state가 가지는 가치를 평가하는 함수였다. 이는 bellman equation에서 유도된 것으로 아래와 같았다.  
$$
V_k(s) = R(s) + \gamma \sum_{s' \in S} P(s' | s)V_{k-1} (s')
$$

이를 MDP 버전에 맞게 바꿀 것이다. 먼저 $\pi$ 가 deterministic한 경우를 생각해보자. 우선, reward는 state뿐만 아니라 action의 영향도 받는다. 그리고, transition model 또한 action의 영향을 받게 된다. 그리고 이 action은 $\pi$가 결정한다. 이 점을 고려하여 다음과 같이 나타낼 수 있을 것이다.  

$$
V_k ^{\pi} (s) = R(s, \pi(s)) + \gamma \sum_{s' \in S} P(s' | s, \pi(s)) V_{k-1}(s')
$$

만약 $\pi$ 가 stochastic하다면 어떨까? 당장 reward function부터 평균의 형태로 바뀌어야 할 것이다. 또한 미래에 받을 보상의 총합 또한, policy에 따른 다음 action의 확률의 영향을 받게 된다. 따라서 다음과 같이 나타낼 수 있을 것이다.  

$$
V_k ^{\pi} (s) = \sum_{a \in A} \pi (a|s) \left[ R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a) V_{k-1}(s')  \right]
$$

이제 모든 state $s$에 대해 $V_0 (s) = 0$으로 두고, 이 과정을 iterate하여, value function이 converge할 때까지 반복하여 value function의 값을 확정할 수 있다. 이 과정을 policy evaluation이라고 한다. 그리고 이 update 식을 bellman backup이라고 부른다. (한 단계 다음 state들의 가치를 이용해 현재 value를 갱신) 이 과정의 complexity는 $O\left( |S|^2 |A| \right)$ 이며, 만약 policy가 deterministic하다면, $O\left( |S|^2 \right)$ 가 된다.  

우리가 찾아야 할 policy는 value를 최대로 만드는 policy일 것이다. 이를 수식으로 표현하면 다음과 같다.  
$$
\pi * (s) = \argmax_{\pi} V^{\pi} (s)
$$

optimal policy는 여러 개일 수 있지만, 당연하게도 optimal value function은 unique하다. 우리는 이 optimal value function이 되도록 하는 policy $\pi*$를 찾으면 된다.  
가장 먼저 떠오르는 방법은 brute-force다. 모든 가능한 policy에 대해 value function을 계산하여 가장 좋은 policy를 찾는 것이다. 문제는 deterministic policy의 개수가 $|A|^{|S|}$라는 것이다. 이 많은 policy를 brute-force하는 것은 매우 비효율적이다. 따라서 우리는 다음 두 가지 과정을 통해 최적의 policy를 찾아볼 것이다.  

먼저 policy iteration (PI)에 대해 살펴보자.  

> **Policy Iteration**  
> <br>
> Set $i = 0$  
> Initialize $\pi_0 (s)$ randomly for all states $s$  
> while $i == 0$ or $|\pi_i - \pi_{i-1}|_1 > 0$:
>
> &emsp;&emsp; $V^{\pi_i}$ <- MDP policy evaluation of $\pi_i$  
> &emsp;&emsp; $\pi_{i+1}$ <- Policy improvement ($\pi_{i+1} (s) = \argmax_{a} Q^{\pi_i}(s,a) \quad \forall s \in S$)  
> &emsp;&emsp; $i = i+1$  

위의 과정에서 아직 모르는 것이 하나 있다. policy improvement가 그것이다. 이제부터 policy improvement가 무엇인지 살펴볼 것이다. 그 전에, 먼저 $Q$-value에 대해 먼저 정의한다.  
$$
Q^{\pi} (s, a) = R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a)V^{\pi} (s')
$$

수식을 살펴보면, 먼저 특정 action을 먼저 취하고, 그 다음 time step부터 policy를 따를 때의 value를 나타내는 것임을 알 수 있다. 이 value를 $Q$-value, 또는 state-action value라고 한다. 이 식을 보면 감이 올텐데, 이제부터 우리는 모든 state에서 각각의 action에 대해, 이 Q-value를 계산하여, 가장 Q-value가 높은 action을 선택하도록 policy를 변경할 것이다. 이 과정을 수식으로 나타내면 다음과 같다.  
$$
\pi_{i+1} (s) = \argmax_{a} Q^{\pi_i} (s, a) \quad \forall s \in S
$$

이 과정은 일종의 greedy algorithm이다. 그렇다면, 이렇게 greedy algorithm으로 policy를 update하는 과정이 과연 optimal policy를 유도할 수 있을까? 이제부터 PI의 opt. policy 수렴성에 대해 증명해보자.  
먼저, policy improvement를 통해 얻은 새로운 policy가 이전의 policy보다 좋은 policy임을 보일 것이다.  
이를 수식으로 나타내면, $V^{\pi_i} (s) \leq \max_{a} Q^{\pi _i} (s, a)$ 가 된다.  
$$
\begin{align}
V^{\pi_i} (s) &\leq \max_{a} Q^{\pi _i} (s, a) \\
              &= max_{a} \left[ R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a)V^{\pi_i}(s') \right] \\
              &= R(s, \pi_{i+1}(s)) + \gamma \sum_{s' \in S} P(s' | s, \pi_{i+1}(s')) V^{\pi_i}(s') \quad (\text{by definition of } \pi_{i+1})  
\end{align}
$$

식 (3)은 새로운 정책 $\pi_{i+1}$ 에 대한 bellman backup을 기존 가치 함수 $V^{\pi_i}$에 적용한 것과 같다. 이 과정은 모든 state $s$에 대하여 성립한다. 우항에 존재하는 value function에 대해 bellman backup을 계속 하면 아래와 같다.

$$V^{\pi_i}(s) \le R(s, \pi_{i+1}(s)) + \gamma \sum_{s'} P(s' | s, \pi_{i+1}(s)) \left[ R(s', \pi_{i+1}(s')) + \gamma \sum_{s''} P(s'' | s', \pi_{i+1}(s')) V^{\pi_i}(s'') \right]$$

이 과정을 계속해서 반복한다. 그러면, 계속해서 더 먼 미래의 value를 사용해 현재의 value function을 나타낼 수 있으며, 결국 $V^{\pi_i}$ 는 $\gamma$ 가 지속적으로 곱해지면서 영향력이 0으로 수렴하게 된다. 결국 이 식은 새로운 정책 $\pi_{i+1}$ 에 대한 value function인 $V^{\pi_{i+1}}(s)$ 로 수렴하게 된다.  

policy improvement 과정이 monotonic improvement함을 보였다. 이제 이 과정이 optimality로 수렴함을 보이면, PI 과정을 통해 최적의 $\pi$ 를 찾을 수 있다는 것이 증명된다.  

먼저, policy가 더 이상 변하지 않는 경우에 대해 생각해보자. 즉, $\forall s \in S, \pi_{i+1} (s) = \pi_{i} (s)$ 인 경우를 생각해보자.  
그러면, $\forall s \in S, Q^{\pi_{i+1}}(s,a) = Q^{\pi_{i}}(s,a)$ 가 성립하게 된다. 이것을 policy improvement step에 대입해보자.  
$$
Q^{\pi_i}(s,a) = R(s, a) + \gamma \sum_{s' \in S} P(s' | s, a)V^{\pi_i}(s') \\
\pi_{i+1} (s) = \argmax_{a} Q^{\pi_i}(s,a) \\
\pi_{i+2} (s) = \argmax_{a} Q^{\pi_{i+1}}(s,a) = \argmax_{a} Q^{\pi_i}(s,a)
$$
가 된다. 즉, 한 번 policy가 변하지 않게 되면, 그 이후로도 policy는 변하지 않게 된다.  
이제 과정이 수렴함은 보였으므로, 과정이 optimality에 수렴하는지 증명하도록 하자.  

일단 $\pi_{k}$ 가 수렴하게 된다면,  
$$
V^{\pi_k}(s) \left( = \max_{a} Q^{\pi_k} (s, a) \right) = \max_{a} \left[ R(s,a) + \gamma \sum_{s' \in S} P(s' | s,a) V^{\pi_k} (s') \right]
$$
위의 식을 만족하게 되며, 이는 곧 bellman optimality equation을 만족하는 것으로, optimality로 converge함이 증명된다.  

<details>
<summary> Bellman operations로 PI 이해하기 </summary>  

Bellman operation은 아래(value iteration)에서 정의합니다.  
특정 policy에 대한 bellman backup operator $B^{\pi}$ 를 다음과 같이 정의하자.  
$$
B^{\pi}V(s) = R^{\pi} (s) + \gamma \sum_{s' \in S} P^{\pi} (s'|s)V(s')  
$$

그러면, policy evaluation은 이 $B^{\pi}$ 연산을 $V$ 가 변하지 않을 때까지 누적시킨것과 동일하다.  
$$
V^{\pi} = B^{\pi}B^{\pi} \cdots B^{\pi} V
$$

이렇게 찾은 Value function을 활용하여, 다음과 같이 policy를 만들 수 있다.  
$$
\pi_{k+1}(s) = \argmax_{a} \left[ R(s,a) + \gamma \sum_{s' \in S} P(s'|s,a)V^{\pi_k} (s) \right]
$$

</details>

### Value Iteration  

이번에는 policy를 evaluation하는 또 다른 방법인 value iteration에 대해 알아보자. policy iteration은 policy의 infinite horizon value를 계산하여 policy를 업데이트했다면, value iteration은, time step을 점점 늘려가며 value function을 계속 업데이트하는 느낌으로 이해하면 될 듯하다.  
value iteration에 대해 자세히 알기 전에, 먼저 bellman equation과 bellman backup operator에 대해 짚고 넘어가자. policy의 value function은 반드시 Bellman equation을 만족해야 한다. Bellman backup operator는 value function에 적용하는 것으로, 더 높은 value 값을 가진 새로운 value function을 return하는 operator이다. 정확히 아래와 같이 정의한다.  
$$
BV(s) = \max_{a}\left[ R(s,a) + \gamma \sum_{s' \in S} P(s' | s, a)V(s') \right]
$$

즉, BV는 모든 state $s$에 대해, value function을 만들 수 있다.  

이제 BV를 사용하여 Value Iteration을 정의해보자.  

> **Value Iteration**  
> <br>
> set $k = 1$  
> initialize $V_0(s) = 0 \forall s \in S$  
> while $|V_{k+1} - V_{k}|_\infin >= \epsilon$:  
> &emsp; for each state $s$:  
> $$
> V_{k+1}(s) = \max_{a}\left[ R(s,a) + \gamma \sum_{s' \in S} P(s' | s, a)V_{k}(s') \right] = BV_k \\
> $$  
> after convergence, extract optimal policy (after $k+1$ steps):  
> $$
> \pi(s) = \argmax_{a} \left[ R(s,a) + \gamma \sum_{s' \in S} P(s' | s, a)V_{k+1}(s') \right]
> $$

이제, value iteration이 converge함을 보이기 위해, Contraction operator를 정의한다. Contraction operator란, 다음 조건을 만족시키는 operator를 말한다.  
$O$가 operator이고, $|x|$는 $x$의 norm일 때,  
$$
|OV - OV'| \leq |V - V'|
$$

value iteration 과정의 수렴성 증명은, bellman backup이 contraction임을 보임으로써 증명할 수 있다. 편의를 위해, $|V - V'| = \max_{s} |V(s) - V'(s)|$, 즉 infinity norm으로 정의하자.  
$$
\begin{align}
|BV_{k} - BV_{j}| &= \left| \max_{a}\left[ R(s,a) + \gamma \sum_{s' \in S} P(s' | s,a)V_{k}(s') \right] - \max_{a'}\left[ R(s,a') + \gamma \sum_{s' \in S} P(s' | s,a')V_{j}(s') \right] \right| \\
                  &\leq \max_{a} \left|  R(s,a) + \gamma \sum_{s' \in S} P(s' | s,a)V_{k}(s') - R(s,a) - \gamma \sum_{s' \in S} P(s' | s,a)V_{j}(s') \right| \\
                  &= \max_{a} \left[ \gamma \sum_{s' \in S} P(s' | s,a) \left| \left( V_{k}(s') - V_{j}(s') \right) \right| \right] \\
                  &= \max_{a} \left[ \gamma \left| \left( V_{k}(s') - V_{j}(s') \right) \right| \sum_{s' \in S} P(s' | s,a)  \right] \\
                  &= \max_{a} \left[ \gamma \left| \left( V_{k}(s') - V_{j}(s') \right) \right| \times 1  \right] \\
                  &= \gamma \left| \left( V_{k}(s') - V_{j}(s') \right) \right| \quad < \left| \left( V_{k}(s') - V_{j}(s') \right) \right| \qquad (\gamma < 1)
\end{align}
$$
